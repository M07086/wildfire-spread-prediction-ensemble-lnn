# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BDJ5mPvxkyXIfynqlFY2KgNSjyeRfxku
"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

## PART  II  --- 1. Imports & Settings ----------------------------------------------------------------
import os
import math, random, itertools

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter, binary_opening, label
from tensorflow.keras.utils import plot_model

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input, backend as K

from sklearn.metrics import confusion_matrix, precision_recall_curve, classification_report, roc_auc_score, auc
from scipy.ndimage import binary_opening, label

# ---------------- Dataset Settings ----------------
train_file_pattern = '/content/drive/MyDrive/wildfire_project/raw_data/next_day_wildfire_spread_train*'
val_file_pattern = '/content/drive/MyDrive/wildfire_project/raw_data/next_day_wildfire_spread_eval*'
test_file_pattern = '/content/drive/MyDrive/wildfire_project/raw_data/next_day_wildfire_spread_test*'

# ---------------Input Features------------------
INPUT_FEATURES = ['elevation', 'th', 'vs', 'tmmn', 'tmmx', 'sph',
                  'pr', 'pdsi', 'NDVI', 'population', 'erc']
PREV_FIRE = 'PrevFireMask'
LABEL = 'FireMask'
#----------------Feature Stats----------------
DATA_STATS = {
    'elevation': (0.0, 3536.0, 896.5714, 842.6101),
    'pdsi': (-6.0559, 6.7432, -0.7729, 2.4407),
    'NDVI': (-3826.  , 9282.  , 5350.6865, 2185.2192),
    'pr': (0.0, 19.2422, 0.3234289, 1.5336641),
    'sph': (0.0, 1.0, 0.0065, 0.0037),
    'th': (0.0, 360.0, 146.6468, 3435.0725),
    'tmmn': (253.15, 299.63, 281.85, 18.49),
    'tmmx': (253.15, 317.39, 297.72, 19.46),
    'vs': (0.0, 9.7368, 3.6278, 1.3092),
    'erc': (0.0, 109.93, 53.47, 25.10),
    'population': (0.0, 2935.75, 30.46, 214.20),
    'PrevFireMask': (-1.0, 1.0, 0.0, 1.0),
    'FireMask': (-1.0, 1.0, 0.0, 1.0)
}

ENSEMBLE_BATCH_SIZE = 32   # smaller batch size for ensemble
DEFAULT_BATCH_SIZE  = 128  # used in single‐model pipelines
EPOCHS = 100

# --- Dataset Preprocessing Functions (normalize, parse) ---
def _normalize(x, key):
    min_val, max_val, mean, std = DATA_STATS[key]
    x = tf.clip_by_value(x, min_val, max_val)
    return (x - mean) / std

def _rescale(x, key):
    min_val, max_val, *_ = DATA_STATS[key]
    x = tf.clip_by_value(x, min_val, max_val)
    return (x - min_val) / (max_val - min_val)


# 1) Parse function for branch-wise input (no masks)
def _parse_fn_ensemble_simple(example_proto):
    # List of features plus prevfiremask and label
    features = INPUT_FEATURES + [PREV_FIRE, LABEL]
    feature_desc = {f: tf.io.FixedLenFeature([64,64], tf.float32) for f in features}
    parsed = tf.io.parse_single_example(example_proto, feature_desc)

    # Each environmental feature, normalized
    output_dict = {}
    for feat in INPUT_FEATURES:
        feat_norm = _normalize(parsed[feat], feat)
        feat_norm = tf.expand_dims(feat_norm, -1)  # (64,64,1)
        output_dict[f"{feat}_input"] = feat_norm

    # PrevFireMask (normalized)
    prev_fire = _normalize(parsed[PREV_FIRE], PREV_FIRE)
    prev_fire = tf.expand_dims(prev_fire, -1)  # (64,64,1)
    output_dict['prevfire_input'] = prev_fire

    # Label (rescaled 0–1)
    label = _rescale(parsed[LABEL], LABEL)
    label = tf.expand_dims(label, -1)
    return output_dict, label


# 2) Dataset loader for simple ensemble
def get_dataset_ensemble_simple(pattern, batch_size=DEFAULT_BATCH_SIZE):
    ds = tf.data.Dataset.list_files(pattern)
    ds = ds.interleave(tf.data.TFRecordDataset, cycle_length=4)
    ds = ds.map(_parse_fn_ensemble_simple, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(tf.data.AUTOTUNE)
    return ds


def prepare_datasets(
    train_pattern,
    val_pattern,
    test_pattern,
    batch_size=DEFAULT_BATCH_SIZE,
    ensemble_flag=False,
    filter_fire=False
):
    """
    Returns (train_ds, val_ds, test_ds).
    - If ensemble_flag=False: uses get_dataset with _parse_fn.
    - If ensemble_flag=True: uses get_dataset_ensemble_simple with _parse_fn_ensemble_simple.
    - filter_fire applies only to the training dataset before concatenation.
    """
    if not ensemble_flag:
        # Single-input pipeline
        fire_ds = get_dataset(train_pattern, filter_fire=True, batch_size=batch_size).take(700 // batch_size)
        non_fire_ds = get_dataset(train_pattern, filter_fire=False, batch_size=batch_size).take(300 // batch_size)
        train_ds = fire_ds.concatenate(non_fire_ds).shuffle(1000 // batch_size).repeat()
        val_ds = get_dataset(val_pattern, filter_fire=False, batch_size=batch_size)
        test_ds = get_dataset(test_pattern, filter_fire=False, batch_size=batch_size)
        return train_ds, val_ds, test_ds
    else:
        # Ensemble-simple pipeline
        def filter_fire_ensemble(inputs_tuple, label):
            cnt = tf.reduce_sum(tf.cast(label > 0.5, tf.int32))
            return cnt > 50

        fire_ds = get_dataset_ensemble_simple(train_pattern, batch_size=batch_size)
        non_fire_ds = get_dataset_ensemble_simple(train_pattern, batch_size=batch_size)
        if filter_fire:
            fire_ds = fire_ds.filter(filter_fire_ensemble)
        fire_ds = fire_ds.take(700 // batch_size)
        non_fire_ds = non_fire_ds.take(300 // batch_size)
        train_ds = fire_ds.concatenate(non_fire_ds).shuffle(1000 // batch_size).repeat()
        val_ds = get_dataset_ensemble_simple(val_pattern, batch_size=batch_size)
        test_ds = get_dataset_ensemble_simple(test_pattern, batch_size=batch_size)
        return train_ds, val_ds, test_ds

def prepare_bootstrap_input(dataset, feature_sets):
    """For each sample: creates a tuple of branch inputs (each shape (64,64,k+1)), and label."""
    def map_fn(x, y):
        branch_inputs = []
        prevfire = x['prevfire_input']
        for feat_group in feature_sets:
            feature_tensors = [x[f"{f}_input"] for f in feat_group]
            concat_feats = tf.concat(feature_tensors + [prevfire], axis=-1)  # (64,64,k+1)
            branch_inputs.append(concat_feats)
        return tuple(branch_inputs), y
    return dataset.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)

# --- Loss Functions ---
def weighted_bce(y_true, y_pred):
    y_true = tf.reshape(y_true, tf.shape(y_pred))
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    bce = K.binary_crossentropy(y_true, y_pred)
    weights = 1 + (20.0 - 1) * y_true
    return tf.reduce_mean(bce * weights)

@tf.keras.utils.register_keras_serializable()
class TverskyLoss(tf.keras.losses.Loss):
    def __init__(self, alpha=0.7, beta=0.3, name='tversky_loss'):
        super().__init__(name=name)
        self.alpha = alpha
        self.beta = beta

    def call(self, y_true, y_pred):
        y_true = tf.reshape(y_true, tf.shape(y_pred))
        y_true_f = tf.reshape(y_true, [-1])
        y_pred_f = tf.clip_by_value(tf.reshape(y_pred, [-1]), 1e-7, 1 - 1e-7)
        tp = tf.reduce_sum(y_true_f * y_pred_f)
        fn = tf.reduce_sum(y_true_f * (1 - y_pred_f))
        fp = tf.reduce_sum((1 - y_true_f) * y_pred_f)
        return 1 - (tp + 1e-7) / (tp + self.alpha * fp + self.beta * fn + 1e-7)

def focal_loss(gamma=2.0):
    def loss(y_true, y_pred):
        y_true = tf.reshape(y_true, tf.shape(y_pred))
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
        return tf.reduce_mean(-(y_true * (1 - y_pred) ** gamma * tf.math.log(y_pred) +
                                 (1 - y_true) * y_pred ** gamma * tf.math.log(1 - y_pred)))
    return loss

#def combined_loss(y_true, y_pred):
#    return 0.3 * weighted_bce(y_true, y_pred) + 0.7 * TverskyLoss(alpha=0.3, beta=0.7)(y_true, y_pred)
# Correction: Return a tf.constant(0.0) in combined_loss when no valid pixels
def combined_loss(y_true, y_pred):
    """
    0.3 * weighted BCE + 0.7 * Tversky, ignoring y_true == -1.
    Handles batches where all pixels are masked out (y_true == -1).
    """
    y_true_flat = tf.reshape(y_true, [-1])
    y_pred_flat = tf.reshape(y_pred, [-1])
    mask = tf.not_equal(y_true_flat, -1)
    y_t_valid = tf.boolean_mask(y_true_flat, mask)
    y_p_valid = tf.boolean_mask(y_pred_flat, mask)
    num_valid = tf.shape(y_t_valid)[0]

    def compute_loss():
        y_t_valid_bce = tf.reshape(y_t_valid, [-1, 1])
        y_p_valid_bce = tf.clip_by_value(tf.reshape(y_p_valid, [-1, 1]), 1e-7, 1 - 1e-7)
        bce = K.binary_crossentropy(y_t_valid_bce, y_p_valid_bce)
        weights = 1 + (5.0 - 1) * y_t_valid_bce
        #bce_loss = tf.reduce_mean(bce * weights)
        bce_loss = tf.reduce_mean(K.binary_crossentropy(y_t_valid_bce, y_p_valid_bce) * weights)

        tp = tf.reduce_sum(y_t_valid * y_p_valid)
        fn = tf.reduce_sum(y_t_valid * (1 - y_p_valid))
        fp = tf.reduce_sum((1 - y_t_valid) * y_p_valid)
        tversky_loss = 1 - (tp + 1e-7) / (tp + 0.3 * fp + 0.7 * fn + 1e-7)
        return 0.3 * bce_loss + 0.7 * tversky_loss

    # tf.cond so the graph is valid, return a tf.constant scalar
    loss = tf.cond(num_valid > 0, compute_loss, lambda: tf.constant(0.0, dtype=tf.float32))
    return loss


# --- Metrics ---
def iou_metric(y_true, y_pred, smooth=1e-6):
    y_true = tf.reshape(y_true, tf.shape(y_pred))
    y_true = tf.reshape(y_true, [-1])
    y_pred = tf.reshape(y_pred, [-1])
    inter = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - inter
    return (inter + smooth) / (union + smooth)


def get_model_size(model):
    """Estimate model size in MB."""
    param_size = 0
    for variable in model.trainable_variables:
        param_size += np.prod(variable.shape.as_list())
    return param_size * 4 / (1024 ** 2)  # 4 bytes per float32

def summarize_and_plot(model, name):
        print(f"\n🔍 Model Summary: {name}")
        model.summary()
        param_count = model.count_params()
        print(f"Total Trainable Parameters: {param_count}")

        # Update the base directory for saving diagrams
        base_save_dir = "/content/drive/MyDrive/wildfire_project/simple"
        diagram_save_path = os.path.join(base_save_dir, "architecture_diagrams")
        os.makedirs(diagram_save_path, exist_ok=True) # Create directory

        # Save model diagram as PNG
        try:
            # Use the updated diagram path
            plot_model(model, to_file=os.path.join(diagram_save_path, f"{name}_architecture.png"), show_shapes=True)
            print(f"Model diagram saved as {os.path.join(diagram_save_path, f'{name}_architecture.png')}")
        except Exception as e:
            print(f"Could not save model diagram: {e}")

        return param_count
# --- ConvNet Model Definitions ---
# --- Prepare ConvNet dataset using all features ---
def combine_features(x, y):
    """Concatenate all per-feature inputs into a single tensor of shape (H, W, 2 * num_features)."""
    feature_tensors = [x[f"{feat}_input"] for feat in INPUT_FEATURES]
    all_feats = tf.concat(feature_tensors, axis=-1)
    return all_feats, y





import tensorflow as tf
from tensorflow.keras import layers, constraints, Model, Input

class LiquidRNNCell(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units

    @property
    def state_size(self):
        return self.units

    def build(self, input_shape):
        self.W_in = self.add_weight(shape=(input_shape[-1], self.units), initializer="random_normal", trainable=True)
        self.W_h = self.add_weight(shape=(self.units, self.units), initializer="random_normal", trainable=True)
        self.tau = self.add_weight(shape=(self.units,), initializer="ones", trainable=True)

    def call(self, inputs, states):
        h = states[0]
        dx = tf.tanh(tf.matmul(inputs, self.W_in) + tf.matmul(h, self.W_h))
        new_h = h + (dx - h) / self.tau
        return new_h, [new_h]

# EnsembleLNN Model
# --- Gate layer for PrevFireMask -----
class PrevFireGate(layers.Layer):
    def __init__(self, init=0.5, **kwargs):
        super().__init__(**kwargs)
        self.init = init
    def build(self, input_shape):
        self.alpha = self.add_weight(
            name="alpha_prev",
            shape=(1,),
            initializer=tf.keras.initializers.Constant(self.init),
            trainable=True,
            constraint=constraints.MinMaxNorm(min_value=0.0, max_value=1.0)
        )
    def call(self, prev):
        return prev * self.alpha  # correct broadcasting


# --- Single LNN branch remains unchanged ---
def build_single_lnn_branch(input_shape=(64,64,3), branch_index=0):
    inp = Input(shape=input_shape, name=f"branch_{branch_index}_input")
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = layers.BatchNormalization()(x)
    skip = layers.Conv2D(32, 1, padding='same', activation='relu')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Reshape((32 * 32, 16))(x)
    x = layers.RNN(LiquidRNNCell(64), return_sequences=True)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(1)(x)
    x = layers.Reshape((32, 32, 1))(x)
    x = layers.UpSampling2D(size=(2, 2))(x)
    x = layers.Concatenate()([x, skip])
    x = layers.Conv2D(1, 1, padding='same')(x)
    x = layers.Activation(tf.nn.sigmoid)(x)
    return Model(inp, x, name=f"branch_model_{branch_index}")


# calibrated_sigmoid stays the same
def calibrated_sigmoid(x):
    return tf.sigmoid(x / 0.5)

# --- Ensemble builder with PrevFireGate integrated ---
def build_custom_ensemble_lnn(feature_subsets):
    inputs = []
    branch_outputs = []
    for i, feat_group in enumerate(feature_subsets):
        in_shape = (64, 64, len(feat_group) + 1)
        inp = Input(shape=in_shape, name=f"branch_{i}_input")
        branch_model = build_single_lnn_branch(input_shape=in_shape, branch_index=i)
        out = branch_model(inp)
        inputs.append(inp)
        branch_outputs.append(out)
    x = layers.Concatenate(axis=-1)(branch_outputs)
    x = layers.Conv2D(1, kernel_size=1, activation=None)(x)
    out = layers.Activation(calibrated_sigmoid)(x)
    model = Model(inputs=inputs, outputs=out, name="custom_elnn")
    model.compile(
        optimizer='adam',
        loss=combined_loss,
        metrics=[
            tf.keras.metrics.AUC(),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall(),
            tf.keras.metrics.BinaryAccuracy(),
            iou_metric
        ]
    )
    return model

#──────────────────────────────────────────────────────────────────────────────
# 9. Hyper‐Parameters for Ensemble Configurations
# ───────────────────────────────────────────────────────────────────────────────
FIXED_BRANCHES   = {2: 15, 3: 15, 4: 15, 5: 15}
FORMULA_BRANCHES = {2: 16, 3: 11, 4: 8, 5: 6}

def generate_random_feature_subsets(input_features, k, n_subsets, seed=42):
    random.seed(seed)
    all_combinations = list(itertools.combinations(input_features, k))
    if len(all_combinations) <= n_subsets:
        return all_combinations
    return random.sample(all_combinations, min(n_subsets, len(all_combinations)))

def generate_fixed_ensemble(input_features, k):
    n_branches = FIXED_BRANCHES[k]
    subsets = generate_random_feature_subsets(input_features, k, n_branches)
    print(f"[Fixed Ensemble] k={k} → branches={len(subsets)}")
    return subsets, len(subsets)

def generate_formula_based_ensemble(input_features, k):
    n_branches = FORMULA_BRANCHES[k]
    subsets = generate_random_feature_subsets(input_features, k, n_branches)
    print(f"[Formula-based Ensemble] k={k} → branches={len(subsets)}")
    return subsets, len(subsets)
# ───────────────────────────────────────────────────────────────────────────────
# 10. Define base_path + subfolder structure
# ───────────────────────────────────────────────────────────────────────────────
base_path        = "/content/drive/MyDrive/wildfire_project/esimple"
os.makedirs(base_path, exist_ok=True)

models_folder    = os.path.join(base_path, "emodels")
histories_folder = os.path.join(base_path, "ehistories")
plots_folder     = os.path.join(base_path, "eplots")

for folder in (models_folder, histories_folder, plots_folder):
    os.makedirs(folder, exist_ok=True)
#
# ───────────────────────────────────────────────────────────────────────────────
# 11. train_model and evaluate_model (identical to before except paths)
# ───────────────────────────────────────────────────────────────────────────────
def train_model(model, train_ds, val_ds, name, steps_per_epoch=5, validation_steps=5):
    """
    Trains `model` on `train_ds`, validates on `val_ds`, and:
      • saves best weights to base_path+"/emodels/{name}/best_model.keras"
      • writes training-history CSV & Pickle under base_path+"/ehistories/{name}_history.*"
    """
    # 11.1 Make directory for this model
    model_dir = os.path.join(models_folder, name)
    os.makedirs(model_dir, exist_ok=True)

    # 11.2 Build callbacks
    checkpoint_filepath = os.path.join(model_dir, "best_model.keras")
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_filepath,
            save_best_only=True,
            monitor="val_loss",
            mode="min",
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            patience=10,
            monitor="val_loss",
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.TensorBoard(
            log_dir=os.path.join(model_dir, "logs")
        )
    ]

    # 11.3 Train
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        steps_per_epoch=steps_per_epoch,
        validation_steps=validation_steps,
        callbacks=callbacks
    )

    # 11.4 Persist history to CSV + Pickle
    hist_dict = history.history
    hist_df   = pd.DataFrame(hist_dict)

    hist_csv_path = os.path.join(histories_folder, f"{name}_history.csv")
    hist_pkl_path = os.path.join(histories_folder, f"{name}_history.pickle")

    hist_df.to_csv(hist_csv_path, index_label="epoch")
    hist_df.to_pickle(hist_pkl_path)

    print(f"✔️ Saved history for '{name}' →\n    • {hist_csv_path}\n    • {hist_pkl_path}")
    return history

def evaluate_model(model, test_ds, steps=5):
    loss, auc_val, precision_val, recall_val, accuracy_val, iou_val = model.evaluate(test_ds, steps=steps)
    print("✅ Test Evaluation Metrics:")
    print(f"  • Loss:      {loss:.4f}")
    print(f"  • AUC:       {auc_val:.4f}")
    print(f"  • Precision: {precision_val:.4f}")
    print(f"  • Recall:    {recall_val:.4f}")
    print(f"  • Accuracy:  {accuracy_val:.4f}")
    print(f"  • IoU:       {iou_val:.4f}")

    return {
        "loss":      loss,
        "auc":       auc_val,
        "precision": precision_val,
        "recall":    recall_val,
        "accuracy":  accuracy_val,
        "iou":       iou_val
    }

# ───────────────────────────────────────────────────────────────────────────────
# 12. plot_model_comparison (bar‐chart of final metrics, saved to disk)
# ───────────────────────────────────────────────────────────────────────────────
def plot_model_comparison(results, save_folder=None):
    """
    Plots and optionally saves bar‐charts for 'precision','recall','iou','auc'
    across all models in `results` dict.
    results: { model_name: { 'precision':…, 'recall':…, 'iou':…, 'auc':… , ...}, … }
    """
    metrics_to_plot = ['precision', 'recall', 'iou', 'auc']
    model_names = list(results.keys())

    for metric in metrics_to_plot:
        if metric in results[model_names[0]]:
            values = [results[m].get(metric, 0.0) for m in model_names]
            plt.figure(figsize=(10, 6))
            bars = plt.bar(model_names, values, color='skyblue')
            plt.title(f"Model Comparison – {metric.upper()}")
            plt.ylabel(metric.upper())
            plt.xticks(rotation=45, ha='right')
            for bar, val in zip(bars, values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f"{val:.3f}",
                         ha='center', va='bottom', fontsize=9)
            plt.grid(axis='y', linestyle='--', alpha=0.6)
            plt.tight_layout()
            if save_folder:
                os.makedirs(save_folder, exist_ok=True)
                png_path = os.path.join(save_folder, f"{metric}_comparison.png")
                plt.savefig(png_path, dpi=200)
                print(f"   • Saved → {png_path}")
            plt.close()


# ───────────────────────────────────────────────────────────────────────────────
# 13. plot_and_save_history: per‐epoch curves (loss, auc, precision, recall, iou)
# ───────────────────────────────────────────────────────────────────────────────
def plot_and_save_history(model_name, history_df, save_folder):
    """
    history_df: DataFrame indexed by epoch, with columns e.g.
      ['loss','val_loss','auc','val_auc','precision','val_precision', … 'iou','val_iou']
    Saves plots under save_folder/{model_name}_{metric}.png
    """
    os.makedirs(save_folder, exist_ok=True)

    metric_pairs = [
        ("loss", "val_loss"),
        ("auc",  "val_auc"),
        ("precision", "val_precision"),
        ("recall",    "val_recall"),
        ("binary_accuracy", "val_binary_accuracy"),
        ("iou", "val_iou"),  # if your key is 'iou' and 'val_iou'
    ]

    for train_metric, val_metric in metric_pairs:
        if train_metric in history_df.columns and val_metric in history_df.columns:
            plt.figure(figsize=(8, 5))
            plt.plot(history_df[train_metric], label=f"train_{train_metric}")
            plt.plot(history_df[val_metric],   label=f"val_{val_metric}")
            plt.title(f"{model_name}: {train_metric} vs {val_metric}")
            plt.xlabel("Epoch")
            plt.ylabel(train_metric)
            plt.legend()
            plt.grid(axis='both', linestyle='--', alpha=0.5)
            plt.tight_layout()

            metric_name = train_metric.replace("val_", "").replace("_", "")
            png_path = os.path.join(save_folder, f"{model_name}_{metric_name}.png")
            plt.savefig(png_path, dpi=200)
            plt.close()
            print(f"   • Saved history plot → {png_path}")

# ───────────────────────────────────────────────────────────────────────────────
# 14. MAIN TRAINING LOOP: Build & train all ensemble variants
# ───────────────────────────────────────────────────────────────────────────────

# ---- MAIN TRAINING LOOP ------------------------------------------
train_ds_ensemble, val_ds_ensemble, test_ds_ensemble = prepare_datasets(
    train_file_pattern,
    val_file_pattern,
    test_file_pattern,
    batch_size=ENSEMBLE_BATCH_SIZE,
    ensemble_flag=True
)

histories     = {}
results       = {}
models_dict   = {}
post_datasets = {}

ensemble_config = {}
for k in [2, 3, 4, 5]:
    fixed_subsets, actual_fixed_branches    = generate_fixed_ensemble(INPUT_FEATURES, k)
    formula_subsets, actual_formula_branches = generate_formula_based_ensemble(INPUT_FEATURES, k)
    ensemble_config[f"fixed_k{k}"]   = {'num_branches': actual_fixed_branches,   'subsets': fixed_subsets}
    ensemble_config[f"formula_k{k}"] = {'num_branches': actual_formula_branches, 'subsets': formula_subsets}

for name, cfg in ensemble_config.items():
    print(f"\n🔁 Training {name}: k={len(cfg['subsets'][0])}, branches={cfg['num_branches']}")
    model = build_custom_ensemble_lnn(cfg['subsets'])
    print(f"\n--- Model Summary for '{name}' ---")
    model.summary()
    param_count = model.count_params()
    print(f"Total trainable parameters: {param_count}")

    train_ds_prepared = prepare_bootstrap_input(train_ds_ensemble, cfg['subsets'])
    val_ds_prepared   = prepare_bootstrap_input(val_ds_ensemble,   cfg['subsets'])
    test_ds_prepared  = prepare_bootstrap_input(test_ds_ensemble,  cfg['subsets'])

    train_steps = max(1, (700 + 300) // ENSEMBLE_BATCH_SIZE // 2)
    val_steps   = max(1, 300 // ENSEMBLE_BATCH_SIZE // 2)
    test_steps  = max(1, 300 // ENSEMBLE_BATCH_SIZE // 2)

    history = train_model(model, train_ds_prepared, val_ds_prepared,
                          name, steps_per_epoch=5,
                          validation_steps=5)
    histories[name] = history.history

    eval_metrics = evaluate_model(model, test_ds_prepared, steps=5)
    eval_metrics['param_count'] = param_count
    results[name] = eval_metrics
    models_dict[name] = model
    post_datasets[name] = test_ds_prepared

# ---- RESULT SAVING AND PLOTTING ------------------------------------------
results_df = pd.DataFrame(results).T
metrics_for_table = [c for c in ["loss","auc","precision","recall","accuracy","iou","param_count"]
                     if c in results_df.columns]

print("\n🏷️ Model comparison table:")
print(results_df[metrics_for_table])

results_csv_path = os.path.join(base_path, "results_comparison.csv")
results_df[metrics_for_table].to_csv(results_csv_path, index_label="model_name")
print(f"✔️ Aggregated results saved to {results_csv_path}")

plot_model_comparison(results, save_folder=os.path.join(plots_folder, "final_metrics"))

for name in histories.keys():
    hist_csv   = os.path.join(histories_folder, f"{name}_history.csv")
    hist_df    = pd.read_csv(hist_csv, index_col="epoch")
    out_folder = os.path.join(plots_folder, "per_model_curves", name)
    plot_and_save_history(name, hist_df, out_folder)

import os
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import (
    precision_recall_curve,
    confusion_matrix,
    ConfusionMatrixDisplay
)
from scipy.ndimage import binary_opening, label

# ----------------- User parameters -----------------
MIN_BLOB_SIZE   = 7   # post‐processing minimum component size
MIN_FIRE_PIXELS = 100  # “fire heaviness” threshold
SAMPLE_COUNT    = 5   # number of samples per model

# ----------------- Helper functions -----------------
def plot_model_comparison(results):
    """Bar charts comparing key metrics for each model."""
    metrics = ['precision','recall','iou','auc']
    names   = list(results.keys())
    for m in metrics:
        vals = [results[n][m] for n in names]
        plt.figure(figsize=(12,8))
        bars = plt.bar(names, vals)
        plt.title(f"Model Comparison – {m.upper()}", fontsize=16, fontweight='bold')
        plt.ylabel(m.upper(), fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right', fontsize=12)
        for b,v in zip(bars, vals):
            plt.text(
                b.get_x()+b.get_width()/2, v,
                f"{v:.2f}", ha='center', va='bottom', fontsize=10, fontweight='bold'
            )
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()


def find_best_threshold(y_true, y_pred, make_plot=True):
    """Compute PR curve, find and plot best-F1 threshold."""
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)
    f1 = 2 * precision * recall / (precision + recall + 1e-7)
    best_idx = np.nanargmax(f1)
    best_thr = float(thresholds[best_idx]) if thresholds.size else 0.5

    if make_plot:
        plt.figure(figsize=(8,6))
        plt.plot(recall, precision, label="PR Curve")
        if thresholds.size:
            plt.scatter(
                recall[best_idx], precision[best_idx],
                c='red', label=f"Best F1 @ {best_thr:.2f}", zorder=5
            )
        plt.xlabel("Recall", fontsize=14, fontweight='bold')
        plt.ylabel("Precision", fontsize=14, fontweight='bold')
        plt.title("Precision–Recall Curve", fontsize=16, fontweight='bold')
        plt.legend(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()

    return precision, recall, thresholds, best_idx, best_thr


def post_process_prediction(pred, threshold, min_blob_size=MIN_BLOB_SIZE):
    """Threshold + morphological opening + remove small components."""
    if pred.ndim == 3 and pred.shape[-1] == 1:
        pred = np.squeeze(pred, -1)
    binary = (pred > threshold).astype(np.uint8)
    cleaned = binary_opening(binary, structure=np.ones((3,3),dtype=np.uint8))
    lbl, num = label(cleaned)
    out = np.zeros_like(cleaned)
    for i in range(1, num+1):
        comp = (lbl == i)
        if comp.sum() >= min_blob_size:
            out[comp] = 1
    return out


def select_samples_with_fire(dataset_batched, num_samples=SAMPLE_COUNT, min_fire_pixels=MIN_FIRE_PIXELS):
    """
    Unbatch and pick the first num_samples with ≥ min_fire_pixels fire pixels.
    """
    selected = []
    for feats, lbl in dataset_batched.unbatch():
        cnt = tf.reduce_sum(tf.cast(lbl > 0.5, tf.int32)).numpy()
        if cnt >= min_fire_pixels:
            selected.append((feats, lbl))
            if len(selected) == num_samples:
                break
    if len(selected) < num_samples:
        print(f"⚠️ Only {len(selected)}/{num_samples} fire-heavy samples found.")
    return selected


# ----------------- Main Evaluation Script -----------------

# Use the datasets you prepared after bootstrapping
test_datasets = post_datasets    # this is a dict: { model_name: test_dataset, ... }

# 1) Overall bar-chart of final metrics
plot_model_comparison(results)

# 2) Per-model deep dive
cm_summary      = {}
pixel_errors    = {}
image_variances = {}

for name, model in models_dict.items():
    print(f"\n📌 Evaluating {name}")
    ds = test_datasets.get(name)
    if ds is None:
        print(f"⚠️ No test dataset for {name}, skipping.")
        continue

    # Gather all true/pred values
    y_true_list, y_pred_list = [], []
    for x, y in ds:
        preds = model.predict(x, verbose=0)[...,0]  # squeeze last dim
        y_true_list.append(y.numpy()[...,0].flatten())
        y_pred_list.append(preds.flatten())

    y_true = np.concatenate(y_true_list).astype(int)
    y_pred = np.concatenate(y_pred_list)

    # PR curve & best threshold
    _, _, _, _, best_thr = find_best_threshold(y_true, y_pred, make_plot=True)
    print(f"Best threshold = {best_thr:.2f}")

    # Confusion summary
    cm_mat = confusion_matrix(y_true, (y_pred > best_thr).astype(int))
    tn, fp, fn, tp = cm_mat.ravel()
    cm_summary[name] = {'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp}

    # Pixel-wise absolute error
    pixel_errors[name] = np.abs(y_pred - y_true)

    # Per-image error variance
    vars_ = []
    for x, y in ds:
        batch_preds = model.predict(x, verbose=0)[...,0]
        truths     = y.numpy()[...,0]
        for i in range(batch_preds.shape[0]):
            err = np.abs(batch_preds[i].flatten() - truths[i].flatten())
            vars_.append(err.var())
    image_variances[name] = vars_

    # Fire-heavy sample visuals
    samples = select_samples_with_fire(ds)
    for idx, (feats, lbl) in enumerate(samples, start=1):
        # build proper input
        if isinstance(feats, dict):
            inp = {k: tf.expand_dims(v,0) for k,v in feats.items()}
            prev_fire = list(feats.values())[0].numpy()[...,1]
        elif isinstance(feats, (list, tuple)):
            inp = [tf.expand_dims(arr,0) for arr in feats]
            prev_fire = feats[0].numpy()[...,1]
        else:
            arr = feats.numpy()
            inp = arr[np.newaxis]
            prev_fire = arr[..., -1]

        gt  = lbl.numpy()[...,0]
        raw = model.predict(inp, verbose=0)[0,...,0]
        post = post_process_prediction(raw, best_thr)

        fig, axs = plt.subplots(1,4,figsize=(24,6))
        titles = ["Prev Fire Mask","Ground Truth","Raw Prediction",f"Post-Processed\n(th={best_thr:.2f})"]
        images = [prev_fire, gt, raw, post]
        cmaps  = ["gray","gray","magma","gray"]
        for ax, img, ttl, cmap in zip(axs,images,titles,cmaps):
            ax.imshow(img, cmap=cmap)
            ax.set_title(f"{name} – Sample {idx}\n{ttl}", fontsize=14)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(cm_mat)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"{name} Confusion Matrix", fontsize=16)
    plt.tight_layout()
    plt.show()

# 3) Confusion summary table
cm_df = pd.DataFrame.from_dict(cm_summary, orient='index', columns=['TN','FP','FN','TP'])
print("\n📋 Confusion Matrix Comparison:\n", cm_df)

# 4) Pixel-error box plot
if pixel_errors:
    plt.figure(figsize=(10,6))
    plt.boxplot(list(pixel_errors.values()),
                tick_labels=list(pixel_errors.keys()),
                showfliers=False)
    plt.title("Pixel-wise Absolute Error across Models",fontsize=16)
    plt.ylabel("Absolute Error",fontsize=14)
    plt.xticks(rotation=45,ha='right')
    plt.grid(axis='y',linestyle='--',alpha=0.7)
    plt.tight_layout()
    plt.show()

# 5) Per-image variance scatter
if image_variances:
    plt.figure(figsize=(10,6))
    for i,(n,vars_) in enumerate(image_variances.items()):
        plt.scatter([i]*len(vars_), vars_, alpha=0.5, s=20, label=n)
    plt.xticks(range(len(image_variances)),image_variances.keys(),rotation=45,ha='right')
    plt.title("Per-Image Error Variance across Models",fontsize=16)
    plt.ylabel("Variance of Pixel Error",fontsize=14)
    plt.legend(fontsize=12,loc='upper right')
    plt.grid(axis='y',linestyle='--',alpha=0.7)
    plt.tight_layout()
    plt.show()

print("\n✅ All plots and tables generated.")

# 5) Per-image variance box plot
if image_variances:
    plt.figure(figsize=(10,6))
    plt.boxplot(
        list(image_variances.values()),
        tick_labels=list(image_variances.keys()),
        showfliers=False
    )
    plt.title("Per-Image Error Variance across Models", fontsize=16, fontweight='bold')
    plt.ylabel("Variance of Pixel Error", fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()