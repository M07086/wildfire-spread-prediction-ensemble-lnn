# -*- coding: utf-8 -*-
"""baseline1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f0FEcdEQps1i1uip3C2wh43by8PJhGGI
"""

import os
import tensorflow as tf
tf.keras.backend.clear_session()
print("TensorFlow version:", tf.__version__)
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# @title
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# PART I
# Includes: ConvNet (WBCE, Focal), Single LNN (WBCE, WBCE+Tversky), Ensemble LNN

# --- Imports and Configs ---
import os
import random
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras import backend as K
import pandas as pd
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter, binary_opening, label
from tensorflow.keras.utils import plot_model
from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay

# ---------------- Dataset Settings ----------------
train_file_pattern = '/content/drive/MyDrive/wildfire_project/raw_data/next_day_wildfire_spread_train*'
val_file_pattern = '/content/drive/MyDrive/wildfire_project/raw_data/next_day_wildfire_spread_eval*'
test_file_pattern = '/content/drive/MyDrive/wildfire_project/raw_data/next_day_wildfire_spread_test*'

# ---------------Input Features------------------
INPUT_FEATURES = ['elevation', 'th', 'vs', 'tmmn', 'tmmx', 'sph',
                  'pr', 'pdsi', 'NDVI', 'population', 'erc']
PREV_FIRE = 'PrevFireMask'
LABEL = 'FireMask'
#----------------Feature Stats----------------
DATA_STATS = {
    'elevation': (0.0, 3536.0, 896.5714, 842.6101),
    'pdsi': (-6.0559, 6.7432, -0.7729, 2.4407),
    'NDVI': (-3826.  , 9282.  , 5350.6865, 2185.2192),
    'pr': (0.0, 19.2422, 0.3234289, 1.5336641),
    'sph': (0.0, 1.0, 0.0065, 0.0037),
    'th': (0.0, 360.0, 146.6468, 3435.0725),
    'tmmn': (253.15, 299.63, 281.85, 18.49),
    'tmmx': (253.15, 317.39, 297.72, 19.46),
    'vs': (0.0, 9.7368, 3.6278, 1.3092),
    'erc': (0.0, 109.93, 53.47, 25.10),
    'population': (0.0, 2935.75, 30.46, 214.20),
    'PrevFireMask': (-1.0, 1.0, 0.0, 1.0),
    'FireMask': (-1.0, 1.0, 0.0, 1.0)
}

DEFAULT_BATCH_SIZE = 128
EPOCHS = 100

# --- Dataset Preprocessing Functions (normalize, parse) ---
def _normalize(x, key):
    min_val, max_val, mean, std = DATA_STATS[key]
    x = tf.clip_by_value(x, min_val, max_val)
    return (x - mean) / std

def _rescale(x, key):
    min_val, max_val, *_ = DATA_STATS[key]
    x = tf.clip_by_value(x, min_val, max_val)
    return (x - min_val) / (max_val - min_val)

def _parse_fn(example_proto):
    features = INPUT_FEATURES + [PREV_FIRE, LABEL]
    feature_desc = {f: tf.io.FixedLenFeature([64, 64], tf.float32) for f in features}
    parsed = tf.io.parse_single_example(example_proto, feature_desc)

    # Concatenate environmental features
    environmental_features = [tf.expand_dims(_normalize(parsed[feat], feat), -1) for feat in INPUT_FEATURES]
    all_environmental_feats = tf.concat(environmental_features, axis=-1) # Shape (64, 64, 11)

    # Normalize and expand PrevFireMask
    prev_fire = _normalize(parsed[PREV_FIRE], PREV_FIRE)
    prev_fire_exp = tf.expand_dims(prev_fire, -1) # Shape (64, 64, 1)

    # Concatenate environmental features with PrevFireMask
    model_input = tf.concat([all_environmental_feats, prev_fire_exp], axis=-1) # Shape (64, 64, 12)

    label = _rescale(parsed[LABEL], LABEL)
    return model_input, tf.expand_dims(label, -1)

# 1) Parse function for branch-wise input (no masks)
def _parse_fn_ensemble_simple(example_proto):
    features = INPUT_FEATURES + [PREV_FIRE, LABEL]
    feature_desc = {f: tf.io.FixedLenFeature([64,64], tf.float32) for f in features}
    parsed = tf.io.parse_single_example(example_proto, feature_desc)

    # PrevFireMask as an extra channel
    prev_fire = _normalize(parsed[PREV_FIRE], PREV_FIRE)
    prev_fire = tf.expand_dims(prev_fire, -1)

    # Build one input tensor per branch
    branch_inputs = []
    for feat in INPUT_FEATURES:
        feat_norm = _normalize(parsed[feat], feat)
        inp = tf.concat([tf.expand_dims(feat_norm, -1), prev_fire], axis=-1)
        branch_inputs.append(inp)

    # Label tensor
    label = _rescale(parsed[LABEL], LABEL)
    label = tf.expand_dims(label, -1)

    # Return a tuple of inputs matching the model signature, plus label
    return tuple(branch_inputs), label

def get_dataset(pattern, filter_fire=False, batch_size=DEFAULT_BATCH_SIZE):
    ds = tf.data.Dataset.list_files(pattern)
    ds = ds.interleave(tf.data.TFRecordDataset, cycle_length=4)
    ds = ds.map(_parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
    if filter_fire:
        ds = ds.filter(lambda x, y: tf.reduce_sum(tf.cast(y > 0.5, tf.float32)) > 50)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(tf.data.AUTOTUNE)
    return ds


# 2) Dataset loader for simple ensemble
def get_dataset_ensemble_simple(pattern, batch_size=DEFAULT_BATCH_SIZE):
    ds = tf.data.Dataset.list_files(pattern)
    ds = ds.interleave(tf.data.TFRecordDataset, cycle_length=4)
    ds = ds.map(_parse_fn_ensemble_simple, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(tf.data.AUTOTUNE)
    return ds


def prepare_datasets(
    train_pattern,
    val_pattern,
    test_pattern,
    batch_size=DEFAULT_BATCH_SIZE,
    ensemble_flag=False,
    filter_fire=False
):
    """
    Returns (train_ds, val_ds, test_ds).
    - If ensemble_flag=False: uses get_dataset with _parse_fn.
    - If ensemble_flag=True: uses get_dataset_ensemble_simple with _parse_fn_ensemble_simple.
    - filter_fire applies only to the training dataset before concatenation.
    """
    if not ensemble_flag:
        # Single-input pipeline
        fire_ds = get_dataset(train_pattern, filter_fire=True, batch_size=batch_size).take(700 // batch_size)
        non_fire_ds = get_dataset(train_pattern, filter_fire=False, batch_size=batch_size).take(300 // batch_size)
        train_ds = fire_ds.concatenate(non_fire_ds).shuffle(1000 // batch_size).repeat()
        val_ds = get_dataset(val_pattern, filter_fire=False, batch_size=batch_size)
        test_ds = get_dataset(test_pattern, filter_fire=False, batch_size=batch_size)
        return train_ds, val_ds, test_ds
    else:
        # Ensemble-simple pipeline
        def filter_fire_ensemble(inputs_tuple, label):
            cnt = tf.reduce_sum(tf.cast(label > 0.5, tf.int32))
            return cnt > 50

        fire_ds = get_dataset_ensemble_simple(train_pattern, batch_size=batch_size)
        non_fire_ds = get_dataset_ensemble_simple(train_pattern, batch_size=batch_size)
        if filter_fire:
            fire_ds = fire_ds.filter(filter_fire_ensemble)
        fire_ds = fire_ds.take(700 // batch_size)
        non_fire_ds = non_fire_ds.take(300 // batch_size)
        train_ds = fire_ds.concatenate(non_fire_ds).shuffle(1000 // batch_size).repeat()
        val_ds = get_dataset_ensemble_simple(val_pattern, batch_size=batch_size)
        test_ds = get_dataset_ensemble_simple(test_pattern, batch_size=batch_size)
        return train_ds, val_ds, test_ds



# --- Loss Functions ---
def weighted_bce(y_true, y_pred):
    y_true = tf.reshape(y_true, tf.shape(y_pred))
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    bce = K.binary_crossentropy(y_true, y_pred)
    weights = 1 + (20.0 - 1) * y_true
    return tf.reduce_mean(bce * weights)

@tf.keras.utils.register_keras_serializable()
class TverskyLoss(tf.keras.losses.Loss):
    def __init__(self, alpha=0.7, beta=0.3, name='tversky_loss'):
        super().__init__(name=name)
        self.alpha = alpha
        self.beta = beta

    def call(self, y_true, y_pred):
        y_true = tf.reshape(y_true, tf.shape(y_pred))
        y_true_f = tf.reshape(y_true, [-1])
        y_pred_f = tf.clip_by_value(tf.reshape(y_pred, [-1]), 1e-7, 1 - 1e-7)
        tp = tf.reduce_sum(y_true_f * y_pred_f)
        fn = tf.reduce_sum(y_true_f * (1 - y_pred_f))
        fp = tf.reduce_sum((1 - y_true_f) * y_pred_f)
        return 1 - (tp + 1e-7) / (tp + self.alpha * fp + self.beta * fn + 1e-7)

def focal_loss(gamma=2.0):
    def loss(y_true, y_pred):
        y_true = tf.reshape(y_true, tf.shape(y_pred))
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
        return tf.reduce_mean(-(y_true * (1 - y_pred) ** gamma * tf.math.log(y_pred) +
                                 (1 - y_true) * y_pred ** gamma * tf.math.log(1 - y_pred)))
    return loss

#def combined_loss(y_true, y_pred):
#    return 0.3 * weighted_bce(y_true, y_pred) + 0.7 * TverskyLoss(alpha=0.3, beta=0.7)(y_true, y_pred)
def combined_loss(y_true, y_pred):
    """
    0.3 * weighted BCE + 0.7 * Tversky, ignoring y_true == -1.
    Handles batches where all pixels are masked out (y_true == -1).
    """
    y_true_flat = tf.reshape(y_true, [-1])
    y_pred_flat = tf.reshape(y_pred, [-1])
    mask = tf.not_equal(y_true_flat, -1)
    y_t_valid = tf.boolean_mask(y_true_flat, mask)
    y_p_valid = tf.boolean_mask(y_pred_flat, mask)
    num_valid = tf.shape(y_t_valid)[0]

    def compute_loss():
        y_t_valid_bce = tf.reshape(y_t_valid, [-1, 1])
        y_p_valid_bce = tf.clip_by_value(tf.reshape(y_p_valid, [-1, 1]), 1e-7, 1 - 1e-7)
        bce = K.binary_crossentropy(y_t_valid_bce, y_p_valid_bce)
        weights = 1 + (5.0 - 1) * y_t_valid_bce
        #bce_loss = tf.reduce_mean(bce * weights)
        bce_loss = tf.reduce_mean(K.binary_crossentropy(y_t_valid_bce, y_p_valid_bce) * weights)

        tp = tf.reduce_sum(y_t_valid * y_p_valid)
        fn = tf.reduce_sum(y_t_valid * (1 - y_p_valid))
        fp = tf.reduce_sum((1 - y_t_valid) * y_p_valid)
        tversky_loss = 1 - (tp + 1e-7) / (tp + 0.3 * fp + 0.7 * fn + 1e-7)
        return 0.3 * bce_loss + 0.7 * tversky_loss

    # tf.cond so the graph is valid, return a tf.constant scalar
    loss = tf.cond(num_valid > 0, compute_loss, lambda: tf.constant(0.0, dtype=tf.float32))
    return loss

# --- Metrics ---
def iou_metric(y_true, y_pred, smooth=1e-6):
    y_true = tf.reshape(y_true, tf.shape(y_pred))
    y_true = tf.reshape(y_true, [-1])
    y_pred = tf.reshape(y_pred, [-1])
    inter = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - inter
    return (inter + smooth) / (union + smooth)


def get_model_size(model):
    """Estimate model size in MB."""
    param_size = 0
    for variable in model.trainable_variables:
        param_size += np.prod(variable.shape.as_list())
    return param_size * 4 / (1024 ** 2)  # 4 bytes per float32

def summarize_and_plot(model, name):
        print(f"\nüîç Model Summary: {name}")
        model.summary()
        param_count = model.count_params()
        print(f"Total Trainable Parameters: {param_count}")

        # Update the base directory for saving diagrams
        base_save_dir = "/content/drive/MyDrive/wildfire_project/simple"
        diagram_save_path = os.path.join(base_save_dir, "architecture_diagrams")
        os.makedirs(diagram_save_path, exist_ok=True) # Create directory

        # Save model diagram as PNG
        try:
            # Use the updated diagram path
            plot_model(model, to_file=os.path.join(diagram_save_path, f"{name}_architecture.png"), show_shapes=True)
            print(f"Model diagram saved as {os.path.join(diagram_save_path, f'{name}_architecture.png')}")
        except Exception as e:
            print(f"Could not save model diagram: {e}")

        return param_count
# --- ConvNet Model Definitions ---
# --- Prepare ConvNet dataset using all features ---
def combine_features(x, y):
    """Concatenate all per-feature inputs into a single tensor of shape (H, W, 2 * num_features)."""
    feature_tensors = [x[f"{feat}_input"] for feat in INPUT_FEATURES]
    all_feats = tf.concat(feature_tensors, axis=-1)
    return all_feats, y

def build_convnet_model(loss_type='wbce'):
    inputs = Input(shape=(64, 64, len(INPUT_FEATURES)+1), name="conv_input")
    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D()(x)
    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D()(x)
    x = layers.Conv2D(1, 1, activation='sigmoid')(x)
    model = Model(inputs, x)

    if loss_type == 'wbce':
        loss_fn = weighted_bce
    elif loss_type == 'focal':
        loss_fn = focal_loss(gamma=2.0)
    else:
        raise ValueError("Unsupported loss type")

    model.compile(
        optimizer='adam',
        loss=loss_fn,
        metrics=[
            tf.keras.metrics.AUC(),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall(),
            tf.keras.metrics.BinaryAccuracy(),
            iou_metric
        ]
    )
    return model

#Multi-Kernel CNN ----------------------------------------------------

def build_multi_kernel_cnn_model(loss_type='wbce'):
    inp = Input(shape=(64,64, len(INPUT_FEATURES)+1), name="mkc_input")
    # three parallel conv paths
    c3 = layers.Conv2D(32, 3, padding='same', activation='relu')(inp)
    c5 = layers.Conv2D(32, 5, padding='same', activation='relu')(inp)
    c7 = layers.Conv2D(32, 7, padding='same', activation='relu')(inp)
    x = layers.Concatenate()([c3,c5,c7])
    x = layers.MaxPooling2D()(x)

    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D()(x)
    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)
    x = layers.Conv2D(1, 1, activation='sigmoid')(x)

    model = Model(inp, x)
    loss_fn = weighted_bce if loss_type=='wbce' else focal_loss()
    model.compile(optimizer='adam',
                  loss=loss_fn,
                  metrics=[tf.keras.metrics.AUC(),
                           tf.keras.metrics.Precision(),
                           tf.keras.metrics.Recall(),
                           tf.keras.metrics.BinaryAccuracy(),
                           iou_metric])
    return model

# U-Net ----------------------------------------------------------------

def build_unet_model(loss_type='wbce'):
    def conv_block(x, filters):
        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
        return x

    inp = Input((64,64, len(INPUT_FEATURES)+1), name="unet_input")
    # encoder
    c1 = conv_block(inp, 32)
    p1 = layers.MaxPooling2D()(c1)
    c2 = conv_block(p1, 64)
    p2 = layers.MaxPooling2D()(c2)
    # bottleneck
    b = conv_block(p2, 128)
    # decoder
    u1 = layers.UpSampling2D()(b)
    u1 = layers.Concatenate()([u1, c2]) # Skip connection
    c3 = conv_block(u1, 64)
    u2 = layers.UpSampling2D()(c3)
    u2 = layers.Concatenate()([u2, c1]) # Skip connection
    c4 = conv_block(u2, 32)

    out = layers.Conv2D(1, 1, activation='sigmoid')(c4)
    model = Model(inp, out)

    loss_fn = weighted_bce if loss_type=='wbce' else focal_loss()
    model.compile(optimizer='adam',
                  loss=loss_fn,
                  metrics=[tf.keras.metrics.AUC(),
                           tf.keras.metrics.Precision(),
                           tf.keras.metrics.Recall(),
                           tf.keras.metrics.BinaryAccuracy(),
                           iou_metric])
    return model


# --- Single LNN Model Definition ---
class LiquidRNNCell(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units

    @property
    def state_size(self):
        return self.units

    def build(self, input_shape):
        self.W_in = self.add_weight(shape=(input_shape[-1], self.units), initializer="random_normal", trainable=True)
        self.W_h = self.add_weight(shape=(self.units, self.units), initializer="random_normal", trainable=True)
        self.tau = self.add_weight(shape=(self.units,), initializer="ones", trainable=True)

    def call(self, inputs, states):
        h = states[0]
        dx = tf.tanh(tf.matmul(inputs, self.W_in) + tf.matmul(h, self.W_h))
        new_h = h + (dx - h) / self.tau
        return new_h, [new_h]

def build_single_lnn_model(loss_type='wbce'):
    # match your 22-channel dataset
    n_ch = len(INPUT_FEATURES)+1
    inp = Input(shape=(64, 64, n_ch), name="lnn_combined_input")
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = layers.BatchNormalization()(x)
    skip = layers.Conv2D(32, 1, padding='same', activation='relu')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Reshape((32 * 32, 16))(x)
    x = layers.RNN(LiquidRNNCell(32), return_sequences=True)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(1)(x)
    x = layers.Reshape((32, 32, 1))(x)
    x = layers.UpSampling2D(size=(2, 2))(x)
    x = layers.Concatenate()([x, skip])
    x = layers.Conv2D(1, 1, padding='same')(x)
    x = layers.Activation(tf.nn.sigmoid)(x)
    model = Model(inp, x)

    if loss_type == 'wbce':
        loss_fn = weighted_bce
    elif loss_type == 'combined':
        loss_fn = combined_loss
    else:
        raise ValueError("Unsupported loss type")

    model.compile(
        optimizer='adam',
        loss=loss_fn,
        metrics=[
            tf.keras.metrics.AUC(),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall(),
            tf.keras.metrics.BinaryAccuracy(),
            iou_metric
        ]
    )
    return model

import tensorflow as tf
from tensorflow.keras import layers, constraints, Model, Input


# EnsembleLNN Model
# --- Gate layer for PrevFireMask ---
import tensorflow as tf
from tensorflow.keras import layers, constraints

class PrevFireGate(layers.Layer):
    def __init__(self, init=0.5, **kwargs):
        super().__init__(**kwargs)
        # one-element vector Œ± ‚àà [0,1], initialized to [init]
        self.alpha = self.add_weight(
            name="alpha_prev",
            shape=(1,),  # 1-D tensor instead of scalar
            initializer=tf.keras.initializers.Constant([init]),
            trainable=True,
            constraint=constraints.MinMaxNorm(min_value=0.0, max_value=1.0)
        )

    def call(self, prev):
        # prev: (B, H, W, 1), alpha will broadcast correctly
        return prev * self.alpha


# --- Single LNN branch remains unchanged ---
def build_single_lnn_branch():
    inp = Input(shape=(64, 64, 2))
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = layers.BatchNormalization()(x)
    skip = layers.Conv2D(32, 1, padding='same', activation='relu')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Reshape((32 * 32, 16))(x)
    x = layers.RNN(LiquidRNNCell(32), return_sequences=True)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(1)(x)
    x = layers.Reshape((32, 32, 1))(x)
    x = layers.UpSampling2D(size=(2, 2))(x)
    x = layers.Concatenate()([x, skip])
    x = layers.Conv2D(1, 1, padding='same')(x)
    x = layers.Activation(tf.nn.sigmoid)(x)
    return Model(inp, x)

# calibrated_sigmoid stays the same
def calibrated_sigmoid(x):
    return tf.sigmoid(x / 0.5)

# --- Ensemble builder with PrevFireGate integrated ---

# 3) Ensemble builder
def build_ensemble_lnn_simple():
    inputs, branch_outputs = [], []
    prev_gate = PrevFireGate(init=0.5)

    for feat in INPUT_FEATURES:
        inp = Input(shape=(64, 64, 2), name=f"{feat}_input")
        feat_ch = layers.Lambda(lambda x: x[..., :1])(inp)
        prev_ch = layers.Lambda(lambda x: x[..., 1:])(inp)
        gated_prev = prev_gate(prev_ch)
        gated_inp = layers.Concatenate()([feat_ch, gated_prev])
        branch_out = build_single_lnn_branch()(gated_inp)
        inputs.append(inp)
        branch_outputs.append(branch_out)

    x = layers.Concatenate(axis=-1)(branch_outputs)   # (B,64,64,N_branches)
    # 1√ó1 conv with negative bias so default is ‚Äúno fire‚Äù
    x = layers.Conv2D(
        1, kernel_size=1, activation=None,
        bias_initializer=tf.keras.initializers.Constant(-1.0)
    )(x)
    out = layers.Activation(lambda x: tf.sigmoid(x / 0.5))(x)

    model = Model(inputs, out, name="ensemble_lnn_simple")
    model.compile(
        optimizer='adam',
        loss=combined_loss,
        metrics=[
            tf.keras.metrics.AUC(),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall(),
            tf.keras.metrics.BinaryAccuracy(),
            iou_metric
        ]
    )
    return model


# --- Training & Evaluation Utilities ---

def train_model(model, train_ds, val_ds, name):
        # Update the base directory for saving models and logs
        base_save_dir = "/content/drive/MyDrive/wildfire_project/simple"
        model_save_path = os.path.join(base_save_dir, "models", name)
        log_dir_path = os.path.join(base_save_dir, "logs", name)

        # Create directories if they don't exist
        os.makedirs(model_save_path, exist_ok=True)
        os.makedirs(log_dir_path, exist_ok=True)

        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(model_save_path, "best_model.keras"), # Use filepath instead of first argument
                save_best_only=True,
                monitor="val_loss",
                mode="min",
                verbose=1
            ),
            tf.keras.callbacks.EarlyStopping(
                patience=10,
                monitor="val_loss",
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.TensorBoard(log_dir=log_dir_path) # Use the updated log path
        ]
        history = model.fit(
            train_ds,
            validation_data=val_ds,
            epochs=EPOCHS,
            steps_per_epoch=5,
            validation_steps=5,
            callbacks=callbacks
        )
        return history

def evaluate_model(model, test_ds, param_count=None):
    # Evaluate using Keras .evaluate (matches compile metrics order)
    loss, auc, precision, recall, accuracy, iou = model.evaluate(test_ds, steps=5)
    print("‚úÖ Test Evaluation Metrics:")
    print(f"Loss: {loss:.4f}")
    print(f"AUC: {auc:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"IoU: {iou:.4f}")
    if param_count:
        print(f"Model Size: {param_count / (1024 ** 2):.2f} MB")

    # Save all metrics, including efficiency ones
    return {
        "loss": loss, "auc": auc, "precision": precision,
        "recall": recall, "accuracy": accuracy, "iou": iou, "params": param_count
    }


conv_train_ds, conv_val_ds, conv_test_ds = prepare_datasets(
    train_file_pattern, val_file_pattern, test_file_pattern,
    batch_size=DEFAULT_BATCH_SIZE, ensemble_flag=False
)
lnn_train_ds, lnn_val_ds, lnn_test_ds = prepare_datasets(
    train_file_pattern, val_file_pattern, test_file_pattern,
    batch_size=DEFAULT_BATCH_SIZE, ensemble_flag=False
)
train_ds_simple, val_ds_simple, test_ds_simple = prepare_datasets(
    train_file_pattern, val_file_pattern, test_file_pattern,
    batch_size=DEFAULT_BATCH_SIZE, ensemble_flag=True, filter_fire=True
)


post_datasets = {
    **{k: conv_test_ds for k in ['MultiKernelCNN','UNet','ConvNet_WBCE','ConvNet_Focal']},
    **{k: lnn_test_ds for k in ['SingleLNN_WBCE','SingleLNN_Combined']},
    **{k: test_ds_simple for k in ['EnsembleLNN']}
}

multi_kernel_cnn = build_multi_kernel_cnn_model()
unet = build_unet_model()
convnet_wbce = build_convnet_model('wbce')
convnet_focal = build_convnet_model('focal')
single_lnn_wbce = build_single_lnn_model('wbce')
single_lnn_combined = build_single_lnn_model('combined')
ensemble_simple = build_ensemble_lnn_simple()

# --- POST-PROCESSING & PLOTTING ---
# 1) assemble dict of trained models:
model_dict = {
    'MultiKernelCNN':       multi_kernel_cnn,
    'UNet':                 unet,
    'ConvNet_WBCE':         convnet_wbce,
    'ConvNet_Focal':        convnet_focal,
    'SingleLNN_WBCE':       single_lnn_wbce,
    'SingleLNN_Combined':   single_lnn_combined,
    'EnsembleLNN':          ensemble_simple
}

# map model names ‚Üí their appropriate test dataset
test_datasets = {
    'MultiKernelCNN':     conv_test_ds,
    'UNet':               conv_test_ds,
    'ConvNet_WBCE':       conv_test_ds,
    'ConvNet_Focal':      conv_test_ds,
    'SingleLNN_WBCE':     lnn_test_ds,
    'SingleLNN_Combined': lnn_test_ds,
    'EnsembleLNN':        test_ds_simple
}



# --- Train and Evaluate All Models, capturing histories ---
histories = {}
results = {}
model_params = {}

# Ensemble LNN (Combined)
print("\n" + "="*30)
print("üöÄ Training Ensemble LNN with Combined Loss")
print("="*30)
param_count = summarize_and_plot(ensemble_simple, 'ensemble_lnn')
model_params[ensemble_simple] = param_count
histories['EnsembleLNN'] = train_model(ensemble_simple, train_ds_simple, val_ds_simple, 'ensemble_lnn')
results   ['EnsembleLNN'] = evaluate_model(ensemble_simple, test_ds_simple)
results['EnsembleLNN']['params'] = model_params[ensemble_simple]

# MultiKernelCNN
print("\n" + "="*30)
print("üöÄ Training MultiKernelCNN with WBCE Loss")
print("="*30)
param_count = summarize_and_plot(multi_kernel_cnn, 'multi_kernel_cnn')
model_params[multi_kernel_cnn] = param_count
histories['MultiKernelCNN'] = train_model(multi_kernel_cnn, conv_train_ds, conv_val_ds, 'multi_kernel_cnn')
results   ['MultiKernelCNN'] = evaluate_model(multi_kernel_cnn, conv_test_ds)
results['MultiKernelCNN']['params'] = model_params[multi_kernel_cnn]

# UNet
print("\n" + "="*30)
print("üöÄ Training UNet with WBCE Loss")
print("="*30)
param_count = summarize_and_plot(unet, 'unet')
model_params[unet] = param_count
histories['UNet'] = train_model(unet, conv_train_ds, conv_val_ds, 'unet')
results   ['UNet'] = evaluate_model(unet, conv_test_ds)
results['UNet']['params'] = model_params[unet]

# ConvNet (WBCE)
print("\n" + "="*30)
print("üöÄ Training ConvNet with WBCE Loss")
print("="*30)
param_count = summarize_and_plot(convnet_wbce, 'convnet_wbce')
model_params[convnet_wbce] = param_count
histories['ConvNet_WBCE'] = train_model(convnet_wbce, conv_train_ds, conv_val_ds, 'convnet_wbce')
results   ['ConvNet_WBCE'] = evaluate_model(convnet_wbce, conv_test_ds)
results['ConvNet_WBCE']['params'] = model_params[convnet_wbce]

# ConvNet (Focal)
print("\n" + "="*30)
print("üöÄ Training ConvNet with Focal Loss")
print("="*30)
param_count = summarize_and_plot(convnet_focal, 'convnet_focal')
model_params[convnet_focal] = param_count
histories['ConvNet_Focal'] = train_model(convnet_focal, conv_train_ds, conv_val_ds, 'convnet_focal')
results   ['ConvNet_Focal'] = evaluate_model(convnet_focal, conv_test_ds)
results['ConvNet_Focal']['params'] = model_params[convnet_focal]

# Single LNN (WBCE)
print("\n" + "="*30)
print("üöÄ Training Single LNN with WBCE Loss")
print("="*30)
param_count = summarize_and_plot(single_lnn_wbce, 'single_lnn_wbce')
model_params[single_lnn_wbce] = param_count
histories['SingleLNN_WBCE'] = train_model(single_lnn_wbce, lnn_train_ds, lnn_val_ds, 'single_lnn_wbce')
results   ['SingleLNN_WBCE'] = evaluate_model(single_lnn_wbce, lnn_test_ds)
results['SingleLNN_WBCE']['params'] = model_params[single_lnn_wbce]

# Single LNN (Combined)
print("\n" + "="*30)
print("üöÄ Training Single LNN with Combined Loss")
print("="*30)
param_count = summarize_and_plot(single_lnn_combined, 'single_lnn_combined')
model_params[single_lnn_combined] = param_count
histories['SingleLNN_Combined'] = train_model(single_lnn_combined, lnn_train_ds, lnn_val_ds, 'single_lnn_combined')
results   ['SingleLNN_Combined'] = evaluate_model(single_lnn_combined, lnn_test_ds)
results['SingleLNN_Combined']['params'] = model_params[single_lnn_combined]


print("üìä Final Evaluation Results:")

# Find best threshold

def find_best_threshold(y_true, y_pred_flat, make_plot=True):
    """
    Find the threshold that maximizes F1 on a precision‚Äìrecall curve.

    Returns:
      thresholds, precision, recall, best_idx, best_threshold
    """
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_flat)
    f1 = 2 * precision * recall / (precision + recall + 1e-7)
    best_idx = np.argmax(f1)
    best_threshold = thresholds[best_idx]

    if make_plot:
        plt.figure(figsize=(6,4))
        plt.plot(recall, precision, label="PR Curve")
        plt.scatter(recall[best_idx], precision[best_idx], c="red", label="Best F1")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precision‚ÄìRecall Curve")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return thresholds, precision, recall, best_idx, best_threshold

def post_process_prediction(pred, threshold=0.6, min_blob_size=3):
    if pred.ndim == 3 and pred.shape[-1] == 1:
        pred = np.squeeze(pred, axis=-1)
    binary_pred = (pred > threshold).astype(np.uint8)
    structure = np.ones((3,) * binary_pred.ndim)
    cleaned = binary_opening(binary_pred, structure=structure)
    labeled, num_features = label(cleaned)
    for i in range(1, num_features + 1):
        if np.sum(labeled == i) < min_blob_size:
            cleaned[labeled == i] = 0
    return cleaned

def evaluate_model_with_postprocessing(model, test_dataset, threshold=0.6, min_blob_size=10):
    y_true_all, y_pred_all = [], []
    for x, y in test_dataset:
        pred = model.predict(x)
        y_true_all.append(y.numpy().flatten())
        y_pred_all.append(pred.flatten())
    y_true_flat = np.clip(np.concatenate(y_true_all), 0, 1).astype(int)
    y_pred_flat = np.concatenate(y_pred_all)
    best_thresh = find_best_threshold(y_true_flat, y_pred_flat)
    for x, y in test_dataset.take(5):
        pred = model.predict(x)[0]
        post_pred = post_process_prediction(pred, threshold=best_thresh, min_blob_size=min_blob_size)
        fig, axs = plt.subplots(1, 3, figsize=(14, 5))
        axs[0].imshow(y[0, ..., 0], cmap="gray")
        axs[0].set_title("Ground Truth")
        axs[1].imshow(pred[..., 0], cmap="plasma")
        axs[1].set_title("Raw Prediction")
        axs[2].imshow(post_pred, cmap="gray")
        axs[2].set_title(f"Post-Processed (Thresh={best_thresh:.2f})")
        plt.tight_layout()
        plt.show()
    y_pred_bin = (y_pred_flat > best_thresh).astype(int)
    cm = confusion_matrix(y_true_flat, y_pred_bin)
    print("üî¢ Confusion Matrix:", cm)
    return best_thresh

# --- Visualization Function ---
def plot_model_comparison(results):
    """
    Plot bar charts comparing key metrics for each model.

    Args:
        results (dict): Dictionary mapping model names to metric dicts.
    """
    metrics_to_plot = ['precision', 'recall', 'iou', 'auc']
    model_names = list(results.keys())
    for metric in metrics_to_plot:
        values = [results[m][metric] for m in model_names]
        plt.figure(figsize=(15, 10))
        bars = plt.bar(model_names, values)
        plt.title(f"Model Comparison - {metric.upper()}")
        plt.ylabel(metric.upper())
        plt.xticks(rotation=45)
        for bar, val in zip(bars, values):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f"{val:.2f}", ha='center', va='bottom')
        plt.grid(True, axis='y')
        plt.tight_layout()
        plt.show()

# --- Define path to save results and plots ---
base_path = "/content/drive/MyDrive/wildfire_project/simple/results" # Update the base path
csv_path = os.path.join(base_path, "wildfire_model_comparison_metrics.csv")
os.makedirs(base_path, exist_ok=True)

# --- Save Results as CSV ---
results_df = pd.DataFrame(results).T
results_df.to_csv(csv_path)
print(f"üìÅ Saved evaluation results to {csv_path}")

# --- Print Summary Table ---
print("\nüìä Initial Evaluation Results:")
print(results_df[["precision", "recall", "iou", "auc", "params"]])

def plot_training_curves(histories, metrics=("loss","val_loss","auc","val_auc")):
    for name, hist in histories.items():
        plt.figure(figsize=(6,4))
        for m in metrics:
            if m in hist.history:
                plt.plot(hist.history[m], label=m)
        plt.title(f"{name} Training Curves")
        plt.xlabel("Epoch")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

plot_training_curves(histories, metrics=['loss','val_loss','iou_metric','val_iou_metric'])

import os
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import (
    precision_recall_curve,
    confusion_matrix,
    ConfusionMatrixDisplay
)
from scipy.ndimage import binary_opening, label

# ----------------- User parameters -----------------
MIN_BLOB_SIZE   = 7   # post‚Äêprocessing minimum component size
MIN_FIRE_PIXELS = 50  # ‚Äúfire heaviness‚Äù threshold
SAMPLE_COUNT    = 5   # number of samples per model

# ----------------- Helper functions -----------------
def plot_model_comparison(results):
    """Bar charts comparing key metrics for each model."""
    metrics = ['precision','recall','iou','auc']
    names   = list(results.keys())
    for m in metrics:
        vals = [results[n][m] for n in names]
        plt.figure(figsize=(12,8))
        bars = plt.bar(names, vals)
        plt.title(f"Model Comparison ‚Äì {m.upper()}", fontsize=16, fontweight='bold')
        plt.ylabel(m.upper(), fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right', fontsize=12)
        for b,v in zip(bars, vals):
            plt.text(
                b.get_x()+b.get_width()/2, v,
                f"{v:.2f}", ha='center', va='bottom', fontsize=10, fontweight='bold'
            )
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()


def find_best_threshold(y_true, y_pred, make_plot=True):
    """Compute PR curve, find and plot best-F1 threshold."""
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)
    f1 = 2 * precision * recall / (precision + recall + 1e-7)
    best_idx = np.nanargmax(f1)
    best_thr = float(thresholds[best_idx]) if thresholds.size else 0.5

    if make_plot:
        plt.figure(figsize=(8,6))
        plt.plot(recall, precision, label="PR Curve")
        if thresholds.size:
            plt.scatter(
                recall[best_idx], precision[best_idx],
                c='red', label=f"Best F1 @ {best_thr:.2f}", zorder=5
            )
        plt.xlabel("Recall", fontsize=14, fontweight='bold')
        plt.ylabel("Precision", fontsize=14, fontweight='bold')
        plt.title("Precision‚ÄìRecall Curve", fontsize=16, fontweight='bold')
        plt.legend(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()

    return precision, recall, thresholds, best_idx, best_thr


def post_process_prediction(pred, threshold, min_blob_size=MIN_BLOB_SIZE):
    """Threshold + morphological opening + remove small components."""
    if pred.ndim == 3 and pred.shape[-1] == 1:
        pred = np.squeeze(pred, -1)
    binary = (pred > threshold).astype(np.uint8)
    cleaned = binary_opening(binary, structure=np.ones((3,3),dtype=np.uint8))
    lbl, num = label(cleaned)
    out = np.zeros_like(cleaned)
    for i in range(1, num+1):
        comp = (lbl == i)
        if np.sum(comp) >= min_blob_size:
            out[comp] = 1
    return out


def select_samples_with_fire(dataset_batched, num_samples=SAMPLE_COUNT, min_fire_pixels=MIN_FIRE_PIXELS):
    """
    Unbatch and pick the first num_samples with ‚â• min_fire_pixels fire pixels.
    """
    selected = []
    for feats, lbl in dataset_batched.unbatch():
        cnt = tf.reduce_sum(tf.cast(lbl > 0.5, tf.int32)).numpy()
        if cnt >= min_fire_pixels:
            selected.append((feats, lbl))
            if len(selected) == num_samples:
                break
    if len(selected) < num_samples:
        print(f"‚ö†Ô∏è Only {len(selected)}/{num_samples} fire-heavy samples found.")
    return selected

# ----------------- Main Script -----------------

# Update test_datasets mapping for simple ensemble key
# Ensure the key matches model_dict entry for simple ensemble

test_datasets = {
    'MultiKernelCNN':     conv_test_ds,
    'UNet':               conv_test_ds,
    'ConvNet_WBCE':       conv_test_ds,
    'ConvNet_Focal':      conv_test_ds,
    'SingleLNN_WBCE':     lnn_test_ds,
    'SingleLNN_Combined': lnn_test_ds,
    # Map simple ensemble under the same key used in model_dict
    'EnsembleLNN':        test_ds_simple
}

# 1) Overall bar-chart
plot_model_comparison(results)

# 2) Per-model evaluation, PR curves, confusion tables, sample plots
cm_summary = {}
pixel_errors = {}
image_variances = {}

for name, model in model_dict.items():
    print(f"\nüìå {name}")

    if name not in test_datasets:
        print(f"‚ö†Ô∏è No test dataset for {name}, skipping.")
        continue

    ds = test_datasets[name]
    y_true_list, y_pred_list = [], []

    # gather predictions
    for x, y in ds:
        if isinstance(x, (list, tuple)):
            preds = model.predict(x, verbose=0)
        else:
            preds = model.predict(x, verbose=0)
        y_true_list.append(y.numpy().flatten())
        y_pred_list.append(preds.flatten())

    y_true = np.concatenate(y_true_list).astype(int)
    y_pred = np.concatenate(y_pred_list)

    # PR curve & threshold
    _, _, _, _, best_thr = find_best_threshold(y_true, y_pred, make_plot=True)
    print(f"Best threshold = {best_thr:.2f}")

    # confusion summary
    cm_mat = confusion_matrix(y_true, (y_pred > best_thr).astype(int))
    tn, fp, fn, tp = cm_mat.ravel()
    cm_summary[name] = {'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp}

    # pixel errors
    pixel_errors[name] = np.abs(y_pred - y_true)

    # per-image variance
    img_vars = []
    for x, y in ds:
        if isinstance(x, (list, tuple)):
            batch_preds = model.predict(x, verbose=0)[...,0]
        else:
            batch_preds = model.predict(x, verbose=0)[...,0]
        truths = y.numpy()[...,0]
        for i in range(batch_preds.shape[0]):
            err = np.abs(batch_preds[i] - truths[i]).flatten()
            img_vars.append(err.var())
    image_variances[name] = img_vars

    # sample plots
    samples = select_samples_with_fire(ds)
    for idx, (feats, lbl) in enumerate(samples, start=1):
        if isinstance(feats, dict):
            inp = {k: tf.expand_dims(v,0) for k,v in feats.items()}
            prev_fire = list(feats.values())[0].numpy()[...,1]
        elif isinstance(feats, (list, tuple)):
            inp = [tf.expand_dims(arr,0) for arr in feats]
            prev_fire = feats[0].numpy()[...,1]
        else:
            arr = feats.numpy()
            inp = arr[np.newaxis]
            prev_fire = arr[..., -1]

        gt  = lbl.numpy()[...,0]
        raw = model.predict(inp, verbose=0)[0,...,0]
        post = post_process_prediction(raw, best_thr)

        fig, axs = plt.subplots(1,4,figsize=(24,6))
        titles = ["Prev Fire Mask","Ground Truth","Raw Prediction",f"Post-Processed\n(th={best_thr:.2f})"]
        images = [prev_fire, gt, raw, post]
        cmaps  = ["gray","gray","magma","gray"]
        for ax, img, ttl, cmap in zip(axs,images,titles,cmaps):
            ax.imshow(img, cmap=cmap)
            ax.set_title(f"{name} ‚Äì Sample {idx}\n{ttl}", fontsize=14)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # confusion matrix plot
    disp = ConfusionMatrixDisplay(cm_mat)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"{name} Confusion Matrix", fontsize=16)
    plt.tight_layout()
    plt.show()

# 3) confusion summary table
cm_df = pd.DataFrame.from_dict(cm_summary, orient='index', columns=['TN','FP','FN','TP'])
print("\nüìã Confusion Matrix Comparison:")
print(cm_df)

# 4) pixel-error box plot
plt.figure(figsize=(10,6))
plt.boxplot(list(pixel_errors.values()), labels=pixel_errors.keys(), showfliers=False)
plt.title("Pixel-wise Absolute Error across Models",fontsize=16)
plt.ylabel("Absolute Error",fontsize=14)
plt.xticks(rotation=45,ha='right')
plt.grid(axis='y',linestyle='--',alpha=0.7)
plt.tight_layout()
plt.show()

# 5) per‚Äêimage error‚Äêvariance boxplot
plt.figure(figsize=(10, 6))

# prepare data and labels
labels = list(image_variances.keys())
data   = [image_variances[name] for name in labels]

# draw boxplot
plt.boxplot(data, labels=labels, showfliers=False)

# formatting
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.title("Per-Image Error Variance Across Models", fontsize=16)
plt.ylabel("Variance of Pixel Error", fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


print("\n‚úÖ All plots and tables generated.")